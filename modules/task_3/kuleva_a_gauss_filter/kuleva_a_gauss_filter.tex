\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{}
\author{}
\date{}
\begin{document}

\begin{titlepage}
    \newpage
    \begin{center}
    {\bfseries МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ РОССИЙСКОЙ ФЕДЕРАЦИИ
Федеральное государственное автономное образовательное учреждение
высшего образования
 \\
    «Национальный исследовательский
Нижегородский государственный университет им. Н.И. Лобачевского»
(ННГУ)
}

    %\begin{center}
     Институт информационных технологий, математики и механики \\
    \end{center}

    \vspace{1.2em}

    \begin{center}
    %\textsc{\textbf{}}
    \Large Отчёт по дисциплине \linebreak «Параллельное программирование» \linebreak на тему: \linebreak
«Линейная фильтрация изображений (горизонтальное разбиение). Ядро Гаусса 3x3.»

    \end{center}

    \vspace{5em}


    \begin{flushright}
                       Выполнил(а):
                       студент(ка) группы 382006-2\linebreakКулёва А.А.\underline{\hspace{3cm}} \linebreak\linebreakПроверил: младший научный сотрудник\linebreakНестеров А.Ю.\underline{\hspace{3cm}} 
    \end{flushright}

    \vspace{\fill}
    
    \begin{center}
    Нижний Новгород\\
    2022
    \end{center}

    \end{titlepage}
    
\maketitle

\part*{Введение}
\paragraph{}Размытие по Гауссу в цифровой обработке изображений — способ размытия изображения с помощью функции Гаусса, названной в честь немецкого математика Карла Фридриха Гаусса. Этот эффект широко используется в графических редакторах для уменьшения шума изображения и снижения детализации. Для ускорении фильтрации изображения можно использовать параллельные вычисления, которые будут задействовать все доступные для вычислений процессы.
\paragraph{} При помощи стандарта организации передачи сообщений MPI можно снизить сложность разработки параллельных программ и увеличить эффективность параллельных вычислений, так как реализации библиотеки MPI имеются практически для каждого типа вычислительной системы.
\part*{Постановка задачи}
\paragraph{} Задача данной лабораторной работы заключается в том, чтобы, применяя навыки и знания в параллельном программировании, реализовать фильтр Гаусса для линейной фильтрацию изображений. Работа будет проходить по следующему плану:
\begin{enumerate}
\item Реализовать программу на языке C++, осуществляющую линейную фильтрацию изображений(c горизонтальным разбиением) с помощью ядра Гаусса 3x3, используя технололгию MPI.
\item Для проверки корректности работы программы написать набор тестов, используя фрэймворк для разработки автоматических тестов [Google Test][gtest].
\item  В результате работы должны быть предоставлены 4 файла:\\
1) main.cpp - гугл тесты для задачи. Количество тестов должно быть 5 или больше;\\
2) gauss\_filter.h - заголовочный файл с прототипами функций; \\
3) gauss\_filter.cpp - реализация задачи;\\
4) CMakeLists.txt - файл для настройки проекта.\\
В ходе  работы необходимо применять систему контроля версий [Git][git].
\item Провести эксперименты с помощью реализованной программы, проанализировать результаты и сделать выводы.
\end{enumerate}

\part*{Описание алгоритма}
\paragraph{}{\large Фильтр Гаусса}
\paragraph{}Фильтр Гаусса - это матричный фильтр, главная часть которого – ядро. Ядро – это матрица коэффициентов, которая покомпонентно умножается на значение пикселей
изображения для получения требуемого результата. Коэффиценты для ядра вычисляются по закону нормального распределения:\\ 
 \begin{center}
{\LARGE $G(x, y) = \frac{1}{2\pi \sigma^2} e ^ {\frac{-x^2 -y^2}{2\sigma^2}}$}\\
 \end{center}
\paragraph{}В программе используется ядро Гаусса 3x3:  
$\begin{pmatrix}
1.0 / 16.0 & 1.0 / 8.0 & 1.0 / 16.0\\
1.0 / 8.0 & 1.0 / 4.0 & 1.0 / 8.0\\
1.0 / 16.0 & 1.0 / 8.0 & 1.0 / 16.0\\
\end{pmatrix}$\\
\paragraph{}Фильтр в коде применяется следующим образом:
\begin{lstlisting}
unsigned char getGauseFilterForPixel(const Matrix& matrix, int x, int y, int sizeX) {
    const int halfCoreSize = coreMatrix.size() / 2;
    double sum = 0.0;
    for (int i = -halfCoreSize; i <= halfCoreSize; i++) {
        for (int j = -halfCoreSize; j <= halfCoreSize; j++) {
            const int currentX = x + i;
            const int currentY = y + j;
            const int linCoords = translateCoordinates(currentX, currentY, sizeX);
            const double coreMatrixElement = coreMatrix[i + halfCoreSize][j + halfCoreSize];
            sum += coreMatrixElement * matrix[linCoords];
        }
    }
    return static_cast<unsigned char>(sum);
}
\end{lstlisting}

\part*{Описание структуры программы}
\paragraph{}В программе реализованы функции:
\begin{enumerate} 
\item Matrix generateImage(int sizeX, int sizeY) - генерирует случайное изображение, представленное в виде матрицы. На вход принимает 2 аргумента типа int - размер изображения.
\item int translateCoordinates(int x, int y, int sizeX) - линеаризует координаты. Принимает на вход 3 аргумента типа int: координаты пикселя и ширину изображения.
\item unsigned char getGauseFilterForPixel(const Matrix\& matrix, int x, int y, int sizeX) - фильтр Гаусса для одного пикселя. 
\item Matrix applyGauseFilterPar(const Matrix\& image, int sizeX, int sizeY)  - функция, реализующая параллельную обработку изображения с помощью фильтра Гаусса. 
\end{enumerate} 




\part*{Описание распараллеливания}
\paragraph{}Распараллеливание происходит при вызове функции applyGauseFilterPar, которая принимает на вход исходное изображение, представленное с помощью матрицы, и его размеры, а возвращает отфильтрованное изображение. Алгоритм работы функции можно описать следующим образом:
\begin{enumerate} 
\item Определяется ранг вызывающего процесса в коммуникаторе и общее число процессов.
\item Вычисляется количество строк, передаваемых каждому процессу, и количество пикселей во всех строках.
\item Исходное изображения передаётся процессам с помощью функции MPI\_Bcast.
\item В зависимости от ранга процесса вычисляются начало и конец строки, с которой он будет работать.
\item Для каждого пикселя строки вычисляется результат работы фильтра Гаусса и происходит процесс линеаризации координат. Это нужно, чтобы хранить матрицу в одномерном виде.
\item Все процессы передают данные в нулевой с помощью функции MPI\_Gather.
\item Нулевой процесс возвращает результат
\end{enumerate} 

\part*{Описание схемы OpenMPI}
\paragraph{} Для выполнения лабораторной работы в программном коде я использовала следующие функции MPI:
\begin{enumerate} 
\item int MPI\_Comm\_size(MPI\_Comm comm, int *size)
\paragraph{}С помощью этой функции можно получить количество процессов в выполняемой параллельной программе. Сomm - коммуникатор (в своей лабораторной работе я использую  MPI\_COMM\_WORLD - коммуникатор создается по умолчанию и представляет все процессы параллельной программы), size - переменная, куда передаётся количество процессов.
\item int MPI\_Comm\_rank(MPI\_Comm comm, int *rank)
\paragraph{}Эта функция используется для определения ранга процесса. Comm - коммуникатор, rank - переменная, куда передаётся ранг процесса.

\item int MPI\_Bcast(void *buf, int count, MPI\_Datatype type, int root, MPI\_Comm comm);\\
- buf – адрес начала буфера, содержащего передаваемые данные;\\
- count – число элементов в буфере;\\
- type - тип данных элементов буфера;\\
- root - номер корневого процесса;\\
- comm - коммуникатор, внутри которого будут передаваться данные.
\paragraph{} Эта функция используется для эффективного широковещательного обмена данными. Функция MPI\_Bcast()передает данные из буфера buf, который содержит count элементов типа type,от корневого процесса root процессам коммуникатора comm.

\item int MPI\_Gather(void *sbuf, intscount, MPI\_Datatypestype,
void *rbuf, intrcount, MPI\_Datatypertype,
introot, MPI\_Commcomm);\\
- sbuf, scount, stype – параметры передаваемого сообщения;\\
- rbuf, rcount, rtype – параметры получаемого сообщения;\\
- root – номер процесса, который должен получить результат;\\
- comm – коммуникатор, внутри которого операция должна быть выполнена.
\paragraph{} Функция MPI\_Gather используется для выполнения операции сбора данных от всех процессов в один.\\
\end{enumerate} 
\part*{Результаты экспериментов}
\paragraph{}Вычислим время работы параллельного алгоритма для случайно сгенерированных изображений размером: 10x10, 100x100, 1000x1000, 2000x2000 с помощью функции MPI\_Wtime(). Будем рассматривать работу алгоритма на количестве процессов от 1 до 5.
\paragraph{}Результаты экспериментов занесены в таблицу. Первая строка содержит размеры изображений. Первый столбец, обозначенный n, содержит количество процессов. На пересечении строки и столбца стоит время работы фильтра в секундах.
\begin{center}
\begin{tabular}{|c | c | c | c | c | c |} 
 \hline 
 Размер изображения/ n & 1 & 2 & 3 & 4 & 5 \\ [0.5ex] 
 \hline
  10x10 & 0.0000227 & 0.0003855 & 0.0006809 & 0.0010352 & 0.0029475 \\
 \hline
  100x100 & 0.000222 & 0.0002928 & 0.0001915 & 0.0067623 & 0.0008647 \\
 \hline
  1000x1000 & 0.0228993 & 0.0224053 & 0.0136624 & 0.0129576 & 0.0154105 \\
 \hline
 2000x2000 & 0.0911955 & 0.050604 & 0.0521244 & 0.0487987 & 0.0483993 \\
[1ex] 
 \hline
\end{tabular}
\end{center}


\part*{Выводы }
\paragraph{} Для маленьких изображений применение параллельного алгоритма не эффективно. Мы можем видеть, что для изображений размера 10x10 и 100x100 время работы увеличивается с увеличением числа процессов. Имеет смысл использовать параллельный алгоритм на изображениях большого размера. Причём с увеличением числа процессов уменьшается время работы алгоритма.  

\part*{Заключение}
\paragraph{}В данной лабораторной работе я реализовала программу, осуществляющую линейную фильтрацию изображений (c горизонтальным разбиением) с помощью ядра Гаусса 3x3, используя технололгию MPI. Я написала тесты для проверки корректности работы алгоритма и провела эксперименты. По их результатам можно сделать вывод о том, что применение параллельного алгоритма к изображениям большого размера позволяет производить линейную фильтрацию более эффективно. Причём, чем больше процессов участвуют в обработке изображения, тем меньше времени уходит на работу алгоритма.

\part*{Литература}
\begin{enumerate} 
\item Википедия: Размытие по Гауссу —  https://w.wiki/6CDb
\item Базовые функции MPI - http://rsusu1.rnd.runnet.ru/tutor/method/m2/page03.html
\item Обработка и анализ изображений - \\http://www.graph.unn.ru/rus/materials/CG/CG04\_ImageProcessing2.pdf
\item Конспекты лекций по параллельному программированию. Гергель В.П.,Сысоев А.В. Кафедра МОСТ
\end{enumerate} 

\part*{Приложение}
\section{}CMakeLists.txt
\begin{lstlisting}
get_filename_component(ProjectId ${CMAKE_CURRENT_SOURCE_DIR} NAME)
enable_testing()

if( USE_MPI )
    if( UNIX )
        set(CMAKE_C_FLAGS  "${CMAKE_CXX_FLAGS} -Wno-uninitialized")
        set(CMAKE_CXX_FLAGS  "${CMAKE_CXX_FLAGS} -Wno-uninitialized")
    endif( UNIX )

    set(ProjectId "${ProjectId}_mpi")
    project( ${ProjectId} )
    message( STATUS "-- " ${ProjectId} )

    file(GLOB_RECURSE ALL_SOURCE_FILES *.cpp *.h)

    set(PACK_LIB "${ProjectId}_lib")
    add_library(${PACK_LIB} STATIC ${ALL_SOURCE_FILES} )

    add_executable( ${ProjectId} ${ALL_SOURCE_FILES} )

    target_link_libraries(${ProjectId} ${PACK_LIB})
    if( MPI_COMPILE_FLAGS )
        set_target_properties( ${ProjectId} PROPERTIES COMPILE_FLAGS "${MPI_COMPILE_FLAGS}" )
    endif( MPI_COMPILE_FLAGS )

    if( MPI_LINK_FLAGS )
        set_target_properties( ${ProjectId} PROPERTIES LINK_FLAGS "${MPI_LINK_FLAGS}" )
    endif( MPI_LINK_FLAGS )
    target_link_libraries( ${ProjectId} ${MPI_LIBRARIES} )
    target_link_libraries(${ProjectId} gtest gtest_main)

    enable_testing()
    add_test(NAME ${ProjectId} COMMAND ${ProjectId})

    if( UNIX )
        foreach (SOURCE_FILE ${ALL_SOURCE_FILES})
            string(FIND ${SOURCE_FILE} ${PROJECT_BINARY_DIR} PROJECT_TRDPARTY_DIR_FOUND)
            if (NOT ${PROJECT_TRDPARTY_DIR_FOUND} EQUAL -1)
                list(REMOVE_ITEM ALL_SOURCE_FILES ${SOURCE_FILE})
            endif ()
        endforeach ()

        find_program(CPPCHECK cppcheck)
        add_custom_target(
                "${ProjectId}_cppcheck" ALL
                COMMAND ${CPPCHECK}
                --enable=warning,performance,portability,information,missingInclude
                --language=c++
                --std=c++11
                --error-exitcode=1
                --template="[{severity}][{id}] {message} {callstack} \(On {file}:{line}\)"
                --verbose
                --quiet
                ${ALL_SOURCE_FILES}
        )
    endif( UNIX )

    SET(ARGS_FOR_CHECK_COUNT_TESTS "")
    foreach (FILE_ELEM ${ALL_SOURCE_FILES})
        set(ARGS_FOR_CHECK_COUNT_TESTS "${ARGS_FOR_CHECK_COUNT_TESTS} ${FILE_ELEM}")
    endforeach ()

    add_custom_target("${ProjectId}_check_count_tests" ALL
            COMMAND "${Python3_EXECUTABLE}"
                ${CMAKE_SOURCE_DIR}/scripts/check_count_tests.py
                ${ProjectId}
                ${ARGS_FOR_CHECK_COUNT_TESTS}
    )
else( USE_MPI )
    message( STATUS "-- ${ProjectId} - NOT BUILD!"  )
endif( USE_MPI )
\end{lstlisting}
\section{} gauss\_filter.cpp
\begin{lstlisting}
  // Copyright 2023 Kuleva Anna
#include <mpi.h>
#include <iostream>
#include <random>
#include <iterator>
#include <algorithm>
#include "../../../modules/task_3/kuleva_a_gauss_filter/gauss_filter.h"

CoreMatrix coreMatrix = {
    {1.0 / 16.0, 1.0 / 8.0, 1.0 / 16.0},
    {1.0 / 8.0, 1.0 / 4.0, 1.0 / 8.0},
    {1.0 / 16.0, 1.0 / 8.0, 1.0 / 16.0}
};

Matrix generateImage(int sizeX, int sizeY) {
    std::mt19937 generator(1);
    std::uniform_int_distribution<> randPixel(0, 255);
    Matrix result;
    result.reserve(sizeX * sizeY);
    for (int i = 0; i < sizeX * sizeY; i++) {
        result.push_back(randPixel(generator));
    }
    return result;
}

int translateCoordinates(int x, int y, int sizeX) {
    return y * sizeX + x;
}

unsigned char getGauseFilterForPixel(const Matrix& matrix, int x, int y, int sizeX) {
    const int halfCoreSize = coreMatrix.size() / 2;
    double sum = 0.0;
    for (int i = -halfCoreSize; i <= halfCoreSize; i++) {
        for (int j = -halfCoreSize; j <= halfCoreSize; j++) {
            const int currentX = x + i;
            const int currentY = y + j;
            const int linCoords = translateCoordinates(currentX, currentY, sizeX);
            const double coreMatrixElement = coreMatrix[i + halfCoreSize][j + halfCoreSize];
            sum += coreMatrixElement * matrix[linCoords];
        }
    }
    return static_cast<unsigned char>(sum);
}

Matrix applyGauseFilterPar(const Matrix& image, int sizeX, int sizeY) {
    int size, rank;
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    const int rowsCount = sizeY - 2;
    const int rowPerOneProc = rowsCount / size + ((rowsCount % size) != 0);
    const int pixelPerOneProcCount = rowPerOneProc * sizeX;

    Matrix tempImage = image;
    tempImage.resize(sizeX * sizeY);
    MPI_Bcast(tempImage.data(), tempImage.size(), MPI_CHAR, 0, MPI_COMM_WORLD);

    const int currentProcBeginRow = rowPerOneProc * rank + 1;
    const int currentProcEndRow = std::min(rowPerOneProc * (rank + 1) + 1, sizeY - 1);

    Matrix procResult;
    procResult.reserve(pixelPerOneProcCount);
    for (int i = currentProcBeginRow; i < currentProcEndRow; i++) {
        for (int j = 0; j < sizeX; j++) {
            const int pos = translateCoordinates(j, i, sizeX);
            if (j == 0 || j == sizeX - 1) {
                procResult.push_back(tempImage[pos]);
                continue;
            }
            const unsigned char newPixelValue = getGauseFilterForPixel(tempImage, j, i, sizeX);
            procResult.push_back(newPixelValue);
        }
    }

    Matrix result;
    if (rank == 0) {
        std::copy(image.begin(), image.begin() + sizeX, std::back_inserter(result));
        result.resize(pixelPerOneProcCount * size + sizeX);
    }

    MPI_Gather(procResult.data(), pixelPerOneProcCount, MPI_CHAR,
        result.data() + sizeX, pixelPerOneProcCount, MPI_CHAR, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        result.resize(sizeX * (sizeY - 1));
        std::copy(image.end() - sizeX, image.end(), std::back_inserter(result));
    }
    return result;
}

\end{lstlisting}

\section{} gauss\_filter.h
\begin{lstlisting}
// Copyright 2023 Kuleva Anna
#ifndef MODULES_TASK_3_KULEVA_A_GAUSS_FILTER_GAUSS_FILTER_H_
#define MODULES_TASK_3_KULEVA_A_GAUSS_FILTER_GAUSS_FILTER_H_

#include <vector>

using Matrix = std::vector<unsigned char>;
using CoreMatrix = std::vector<std::vector<double>>;

Matrix generateImage(int sizeX, int sizeY);
int translateCoordinates(int x, int y, int sizeX);
unsigned char getGauseFilterForPixel(const Matrix& matrix, int x, int y, int sizeX);
Matrix applyGauseFilterPar(const Matrix& image, int sizeX, int sizeY);

#endif  // MODULES_TASK_3_KULEVA_A_GAUSS_FILTER_GAUSS_FILTER_H_

\end{lstlisting}

\section{}main.cpp
\begin{lstlisting}
// Copyright 2023 Kuleva Anna
#include <gtest/gtest.h>
#include "./gauss_filter.h"
#include <gtest-mpi-listener.hpp>


TEST(Gauss_Filter, Image_3x3) {
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    Matrix baseImage;
    Matrix expectedImage;

    if (rank == 0) {
        baseImage = {
            1, 2, 3,
            4, 5, 6,
            7, 8, 9
        };

        expectedImage = baseImage;
    }

    Matrix result = applyGauseFilterPar(baseImage, 3, 3);

    if (rank == 0) {
        ASSERT_EQ(expectedImage, result);
    }
}

TEST(Gauss_Filter, Image_4x3) {
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    Matrix baseImage;
    Matrix expectedImage;

    if (rank == 0) {
        baseImage = {
            20, 2, 3, 40,
            4, 5, 6, 7,
            7, 80, 9, 10
        };

        expectedImage = {
            20, 2, 3, 40,
            4, 15, 12, 7,
            7, 80, 9, 10
        };
    }

    Matrix result = applyGauseFilterPar(baseImage, 4, 3);

    if (rank == 0) {
        ASSERT_EQ(expectedImage, result);
    }
}

TEST(Gauss_Filter, Image_3x4) {
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    Matrix baseImage;
    Matrix expectedImage;

    if (rank == 0) {
        baseImage = {
            1, 2, 3,
            40, 5, 6,
            7, 8, 9,
            10, 11, 120
        };

        expectedImage = {
            1, 2, 3,
            40, 9, 6,
            7, 17, 9,
            10, 11, 120
        };
    }

    Matrix result = applyGauseFilterPar(baseImage, 3, 4);

    if (rank == 0) {
        ASSERT_EQ(expectedImage, result);
    }
}

TEST(Gauss_Filter, Image_4x4) {
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    Matrix baseImage;
    Matrix expectedImage;

    if (rank == 0) {
        baseImage = {
            1, 2, 3, 4,
            5, 6, 7, 80,
            90, 10, 11, 12,
            13, 14, 15, 16
        };

        expectedImage = {
            1, 2, 3, 4,
            5, 11, 16, 80,
            90, 20, 15, 12,
            13, 14, 15, 16
        };
    }

    Matrix result = applyGauseFilterPar(baseImage, 4, 4);

    if (rank == 0) {
        ASSERT_EQ(expectedImage, result);
    }
}

TEST(Gauss_Filter, Image_5x4) {
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    Matrix baseImage;
    Matrix expectedImage;

    if (rank == 0) {
        baseImage = {
            1, 2, 3, 4, 5,
            5, 6, 7, 80, 6,
            90, 10, 11, 12, 7,
            13, 14, 15, 16, 8
        };

        expectedImage = {
            1, 2, 3, 4, 5,
            5, 11, 16, 25, 6,
            90, 20, 15, 19, 7,
            13, 14, 15, 16, 8
        };
    }

    Matrix result = applyGauseFilterPar(baseImage, 5, 4);

    if (rank == 0) {
        ASSERT_EQ(expectedImage, result);
    }
}

int main(int argc, char** argv) {
    ::testing::InitGoogleTest(&argc, argv);
    MPI_Init(&argc, &argv);

    ::testing::AddGlobalTestEnvironment(new GTestMPIListener::MPIEnvironment);
    ::testing::TestEventListeners& listeners =
        ::testing::UnitTest::GetInstance()->listeners();

    listeners.Release(listeners.default_result_printer());
    listeners.Release(listeners.default_xml_generator());

    listeners.Append(new GTestMPIListener::MPIMinimalistPrinter);
    return RUN_ALL_TESTS();
}
\end{lstlisting}
\end{document}
